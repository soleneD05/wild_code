{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; color: black; font-size: 48px;\"><strong>Challenge : prédire le sentiments des tweets (positif ou négatif)</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/solenedegrutere/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/solenedegrutere/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/solenedegrutere/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/solenedegrutere/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>1. Importe ce dataset de tweets dans un DataFrame.</strong></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('nlp/data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>2. Conserve uniquement les tweets positifs et négatifs (donc tu exclus les neutrals). Quel est le pourcentage de tweets positifs/négatifs ?</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage de tweets positifs : 52.44759518425717 %\n",
      "Pourcentage de tweets négatifs : 47.55240481574283 %\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df[df['sentiment'].isin(['positive', 'negative'])]\n",
    "positive_percentage = (df_filtered['sentiment'] == 'positive').mean() * 100\n",
    "negative_percentage = (df_filtered['sentiment'] == 'negative').mean() * 100\n",
    "\n",
    "print(\"Pourcentage de tweets positifs :\",positive_percentage, \"%\")\n",
    "print(\"Pourcentage de tweets négatifs :\",negative_percentage, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>3. Crée une fonction que tu appelleras clean qui prend pour paramètre une phrase (donc un texte str) et qui retourne un texte (str) de tokens après application d'un stemmer ou d'un lemmatizer, séparés par des espaces.</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>4. Récupère la liste des stopwords anglais depuis NLTK, et copie-la dans une liste stopwordsenglish. Complète ta fonction clean pour qu'elle supprime la ponctuation et les stopwords.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test clean() : well well\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwordsenglish = stopwords.words('english')\n",
    "\n",
    "# Fonction pour mapper les tags NLTK vers WordNet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Par défaut\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [word for word in tokens if word not in stopwordsenglish]\n",
    "\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmatized = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in tagged_tokens\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Test rapide\n",
    "print(\"✅ Test clean() :\", clean(\"You are better when I am well.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>5. Applique cette fonction clean à la colonne text de ton DataFrame. Stocke le résultat dans une nouvelle colonne clean du DataFrame. (Le traitement peut durer 2 ou 3 minutes)</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f0/l0pmr7js5m7fqgqkychtm5cw0000gn/T/ipykernel_23535/944896778.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered ['clean'] = df_filtered ['text'].apply(clean)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>bos bullying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>interview leave alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>son put release already buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>2am feeding baby fun smile coo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27474</th>\n",
       "      <td>b78ec00df5</td>\n",
       "      <td>enjoy ur night</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>positive</td>\n",
       "      <td>enjoy ur night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27475</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "      <td>wish could come see u denver husband lose job ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "      <td>wonder rake client make clear .net force devs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "      <td>yay good enjoy break probably need hectic week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16363 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "6      6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "...           ...                                                ...   \n",
       "27474  b78ec00df5                                     enjoy ur night   \n",
       "27475  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27476  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27477  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27478  ed167662a5                         But it was worth it  ****.   \n",
       "\n",
       "                    selected_text sentiment  \\\n",
       "1                        Sooo SAD  negative   \n",
       "2                     bullying me  negative   \n",
       "3                  leave me alone  negative   \n",
       "4                   Sons of ****,  negative   \n",
       "6                             fun  positive   \n",
       "...                           ...       ...   \n",
       "27474                       enjoy  positive   \n",
       "27475                      d lost  negative   \n",
       "27476               , don`t force  negative   \n",
       "27477   Yay good for both of you.  positive   \n",
       "27478  But it was worth it  ****.  positive   \n",
       "\n",
       "                                                   clean  \n",
       "1                                sooo sad miss san diego  \n",
       "2                                       bos bullying ...  \n",
       "3                                  interview leave alone  \n",
       "4                            son put release already buy  \n",
       "6                         2am feeding baby fun smile coo  \n",
       "...                                                  ...  \n",
       "27474                                     enjoy ur night  \n",
       "27475  wish could come see u denver husband lose job ...  \n",
       "27476  wonder rake client make clear .net force devs ...  \n",
       "27477  yay good enjoy break probably need hectic week...  \n",
       "27478                                              worth  \n",
       "\n",
       "[16363 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered ['clean'] = df_filtered ['text'].apply(clean)\n",
    "df_filtered "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>6. Copie la colonne clean dans une Serie X, et la colonne sentiment dans une Serie y. Applique un train-test split avec la taille du jeu d'entrainement à 0.75 avec le random_state = 32.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered ['clean']\n",
    "y = df_filtered ['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>7. Applique un CountVectorizer et entraine des modèles de classification.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Logistic Regression avec CountVectorizer\n",
      "Accuracy (CountVectorizer) : 0.8670251772182841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.87      0.86      1935\n",
      "    positive       0.88      0.87      0.87      2156\n",
      "\n",
      "    accuracy                           0.87      4091\n",
      "   macro avg       0.87      0.87      0.87      4091\n",
      "weighted avg       0.87      0.87      0.87      4091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔎 Logistic Regression avec CountVectorizer\")\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "lr_cv = LogisticRegression(max_iter=1000)\n",
    "lr_cv.fit(X_train_cv, y_train)\n",
    "y_pred_cv = lr_cv.predict(X_test_cv)\n",
    "print(\"Accuracy (CountVectorizer) :\", accuracy_score(y_test, y_pred_cv))\n",
    "print(classification_report(y_test, y_pred_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>8. Applique un TfidfVectorizer et entraine des modèles de classification.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Logistic Regression avec TfidfVectorizer\n",
      "Accuracy (TfidfVectorizer) : 0.8704473233928135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.87      0.86      1935\n",
      "    positive       0.88      0.87      0.88      2156\n",
      "\n",
      "    accuracy                           0.87      4091\n",
      "   macro avg       0.87      0.87      0.87      4091\n",
      "weighted avg       0.87      0.87      0.87      4091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔎 Logistic Regression avec TfidfVectorizer\")\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "lr_tfidf = LogisticRegression(max_iter=1000)\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "print(\"Accuracy (TfidfVectorizer) :\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\"><strong>Bonus : maintenant c'est à toi d'améliorer ton modèle :</strong></p>\n",
    "\n",
    "<p style=\"color: black;\"> - En cherchant des paramètres de modèles : par gridsearch et crossvalidation par exemple ;</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "✅ Meilleurs paramètres : {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'liblinear', 'tfidf__max_df': 0.9, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "✅ Accuracy après GridSearch : 0.8687362503055488\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.87      0.86      1935\n",
      "    positive       0.88      0.87      0.87      2156\n",
      "\n",
      "    accuracy                           0.87      4091\n",
      "   macro avg       0.87      0.87      0.87      4091\n",
      "weighted avg       0.87      0.87      0.87      4091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline vectorisation + classification\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Grille d'hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Unigrammes ou bigrammes\n",
    "    'tfidf__min_df': [1, 3],\n",
    "    'tfidf__max_df': [0.9, 1.0],\n",
    "    'clf__C': [0.1, 1, 10],  # Régularisation\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__solver': ['liblinear']  # adapté pour petits datasets\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Résultats\n",
    "print(\"✅ Meilleurs paramètres :\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"✅ Accuracy après GridSearch :\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: black;\">- En changeant la préparation du texte : par exemple certaines ponctuations peuvent aider le modèle, le point d'exclamation notamment.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean2(text):\n",
    "    text = text.lower()\n",
    "    keep = ['!', '?']\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Conserver ponctuation expressive (!, ?) mais supprimer le reste\n",
    "    tokens = [t for t in tokens if (t in keep or t not in string.punctuation)]\n",
    "    tokens = [t for t in tokens if t not in stopwordsenglish]\n",
    "    \n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
    "    return ' '.join(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f0/l0pmr7js5m7fqgqkychtm5cw0000gn/T/ipykernel_23535/564796572.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered ['clean_2'] = df_filtered ['text'].apply(clean2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>sooo sad miss san diego ! ! !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>bos bullying ...</td>\n",
       "      <td>bos bullying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>interview leave alone</td>\n",
       "      <td>interview ! leave alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>son put release already buy</td>\n",
       "      <td>son put release already buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>2am feeding baby fun smile coo</td>\n",
       "      <td>2am feeding baby fun smile coo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27474</th>\n",
       "      <td>b78ec00df5</td>\n",
       "      <td>enjoy ur night</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>positive</td>\n",
       "      <td>enjoy ur night</td>\n",
       "      <td>enjoy ur night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27475</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "      <td>wish could come see u denver husband lose job ...</td>\n",
       "      <td>wish could come see u denver husband lose job ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "      <td>wonder rake client make clear .net force devs ...</td>\n",
       "      <td>wonder rake client make clear .net force devs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "      <td>yay good enjoy break probably need hectic week...</td>\n",
       "      <td>yay good enjoy break probably need hectic week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>worth</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16363 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "6      6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "...           ...                                                ...   \n",
       "27474  b78ec00df5                                     enjoy ur night   \n",
       "27475  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27476  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27477  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27478  ed167662a5                         But it was worth it  ****.   \n",
       "\n",
       "                    selected_text sentiment  \\\n",
       "1                        Sooo SAD  negative   \n",
       "2                     bullying me  negative   \n",
       "3                  leave me alone  negative   \n",
       "4                   Sons of ****,  negative   \n",
       "6                             fun  positive   \n",
       "...                           ...       ...   \n",
       "27474                       enjoy  positive   \n",
       "27475                      d lost  negative   \n",
       "27476               , don`t force  negative   \n",
       "27477   Yay good for both of you.  positive   \n",
       "27478  But it was worth it  ****.  positive   \n",
       "\n",
       "                                                   clean  \\\n",
       "1                                sooo sad miss san diego   \n",
       "2                                       bos bullying ...   \n",
       "3                                  interview leave alone   \n",
       "4                            son put release already buy   \n",
       "6                         2am feeding baby fun smile coo   \n",
       "...                                                  ...   \n",
       "27474                                     enjoy ur night   \n",
       "27475  wish could come see u denver husband lose job ...   \n",
       "27476  wonder rake client make clear .net force devs ...   \n",
       "27477  yay good enjoy break probably need hectic week...   \n",
       "27478                                              worth   \n",
       "\n",
       "                                                 clean_2  \n",
       "1                          sooo sad miss san diego ! ! !  \n",
       "2                                       bos bullying ...  \n",
       "3                                interview ! leave alone  \n",
       "4                            son put release already buy  \n",
       "6                         2am feeding baby fun smile coo  \n",
       "...                                                  ...  \n",
       "27474                                     enjoy ur night  \n",
       "27475  wish could come see u denver husband lose job ...  \n",
       "27476  wonder rake client make clear .net force devs ...  \n",
       "27477  yay good enjoy break probably need hectic week...  \n",
       "27478                                              worth  \n",
       "\n",
       "[16363 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered ['clean_2'] = df_filtered ['text'].apply(clean2)\n",
    "df_filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'clean2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/wild_code/nlp/wild_nlp_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clean2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf_filtered\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df_filtered [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/wild_code/nlp/wild_nlp_env/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Desktop/wild_code/nlp/wild_nlp_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clean2'"
     ]
    }
   ],
   "source": [
    "X = df_filtered ['clean2']\n",
    "y = df_filtered ['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "✅ Meilleurs paramètres : {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'liblinear', 'tfidf__max_df': 0.9, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "✅ Accuracy après GridSearch : 0.8687362503055488\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.87      0.86      1935\n",
      "    positive       0.88      0.87      0.87      2156\n",
      "\n",
      "    accuracy                           0.87      4091\n",
      "   macro avg       0.87      0.87      0.87      4091\n",
      "weighted avg       0.87      0.87      0.87      4091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline vectorisation + classification\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Grille d'hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Unigrammes ou bigrammes\n",
    "    'tfidf__min_df': [1, 3],\n",
    "    'tfidf__max_df': [0.9, 1.0],\n",
    "    'clf__C': [0.1, 1, 10],  # Régularisation\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__solver': ['liblinear']  # adapté pour petits datasets\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Résultats\n",
    "print(\"✅ Meilleurs paramètres :\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"✅ Accuracy après GridSearch :\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wild_nlp_env)",
   "language": "python",
   "name": "wild_nlp_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
